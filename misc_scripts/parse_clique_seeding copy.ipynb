{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiments with b = 1:\n",
      "  Experiments with N = 500:\n",
      "    Experiment data: [7, 0.906, 35.93760299682617, 0.257, 5.489, 2.091, 28.1]\n",
      "    Experiment data: [6, 0.9004, 31.02330780029297, 0.249, 5.372, 1.797, 23.605]\n",
      "    Experiment data: [7, 1.8056, 47.28633213043213, 4.875, 9.883, 4.891, 27.637]\n",
      "    Experiment data: [6, 0.8998, 31.65418314933777, 0.259, 5.356, 2.083, 23.957]\n",
      "    Experiment data: [7, 0.9054, 34.84461784362793, 0.243, 5.492, 1.803, 27.306]\n",
      "    Experiment data: [7, 1.791, 48.87072515487671, 4.633, 9.611, 6.653, 27.973000000000003]\n",
      "    Experiment data: [7, 1.7915999999999999, 47.58618235588074, 4.543, 9.678, 5.436999999999999, 27.929000000000002]\n",
      "    Experiment data: [6, 0.8994, 30.609424829483032, 0.229, 5.446, 1.766, 23.169]\n",
      "    Experiment data: [7, 1.7768000000000002, 47.413111209869385, 4.602, 9.1, 6.6049999999999995, 27.106]\n",
      "    Experiment data: [7, 0.9208, 34.941853761672974, 0.224, 5.597, 1.679, 27.442]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-399cbbf25198>:113: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  np.sum(len(g) for g in data['rb']), coverage , data['ttotal'], tsample, tvis, tmhs, tregions])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "def parse_logfile(filename):\n",
    "    logfile = open(filename, 'r')\n",
    "    lines = logfile.readlines()\n",
    "    logfile.close()\n",
    "\n",
    "    iteration_data = []\n",
    "    current_iteration = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('ITERATION:'):\n",
    "            if current_iteration:\n",
    "                iteration_data.append(current_iteration)\n",
    "                current_iteration = {}\n",
    "        elif line.startswith('summary'):\n",
    "            current_iteration['summary'] = line.strip()\n",
    "        elif line.startswith('number of regions step'):\n",
    "            current_iteration['number_of_regions_step'] = int(line.split()[-1])\n",
    "        elif line.startswith('number of regions total'):\n",
    "            current_iteration['number_of_regions_total'] = int(line.split()[-1])\n",
    "        elif line.startswith('tstep'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tstep'] = float(a.split()[1])\n",
    "            current_iteration['t_total'] = float(a.split()[3])\n",
    "        elif line.startswith('tsample'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tsample'] = float(a.split()[1])\n",
    "            current_iteration['t_visgraph'] = float(a.split()[3])\n",
    "            current_iteration['t_mhs'] = float(a.split()[5])\n",
    "            current_iteration['t_regions'] = float(a.split()[7])\n",
    "\n",
    "        # elif line.startswith('t_mhs'):\n",
    "        #     current_iteration['t_mhs'] = float(line.split()[1])\n",
    "        # elif line.startswith('t_regions'):\n",
    "        #     current_iteration['t_regions'] = float(line.split()[1])\n",
    "        elif line.startswith('coverage'):\n",
    "            current_iteration['coverage'] = float(line.split()[1])\n",
    "        elif '[VisSeeder] Coverage met, terminated' in line:\n",
    "            current_iteration['termination'] = line.strip()\n",
    "\n",
    "    if current_iteration:\n",
    "        iteration_data.append(current_iteration)\n",
    "    return iteration_data\n",
    "\n",
    "\n",
    "logs_directory = '../logs'\n",
    "\n",
    "# Get a list of all subdirectories in the logs directory\n",
    "experiment_directories = [directory for directory in glob.glob(os.path.join(logs_directory, '*')) if os.path.isdir(directory)]\n",
    "\n",
    "# Define a dictionary to store the grouped experiments\n",
    "grouped_experiments = {}\n",
    "\n",
    "# Iterate over each experiment directory\n",
    "for experiment_directory in experiment_directories:\n",
    "    # Parse the experiment name\n",
    "    experiment_name = os.path.basename(experiment_directory)\n",
    "    name_parts = experiment_name.split('_')\n",
    "    \n",
    "    # Extract the relevant values from the experiment name\n",
    "    if '3dof' in experiment_name:\n",
    "        b = 1  #int(name_parts[-5])\n",
    "        # print(experiment_name)\n",
    "    else:\n",
    "        continue\n",
    "    N = int(name_parts[-2])\n",
    "    \n",
    "    # Get the last pickle file in the 'data' subdirectory\n",
    "    data_directory = os.path.join(experiment_directory, 'data')\n",
    "    pickle_files = glob.glob(os.path.join(data_directory, '*.pkl'))\n",
    "    pkl_files = os.listdir(data_directory)\n",
    "    pkl_idx = np.argsort([int(s.replace('it_', '').replace('.pkl', '')) for s in pkl_files])\n",
    "    \n",
    "    #pickle_files.sort()\n",
    "    \n",
    "    last_pickle_file = data_directory+'/'+pkl_files[pkl_idx[-1]]#pickle_files[-1]#max(pickle_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the last pickle file\n",
    "    with open(last_pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Find the 'summar' subdirectory and get the path to the text file\n",
    "    summar_directory = os.path.join(experiment_directory, 'summary')\n",
    "    text_files = glob.glob(os.path.join(summar_directory, '*.txt'))\n",
    "    last_text_file = max(text_files, key=os.path.getctime)\n",
    "    \n",
    "\n",
    "    iteration_dictionaries = parse_logfile(last_text_file)\n",
    "    tsample = np.sum([i['tsample'] for i in iteration_dictionaries[1:]])\n",
    "    tvis = np.sum([i['t_visgraph'] for i in iteration_dictionaries[1:]])\n",
    "    tmhs = np.sum([i['t_mhs'] for i in iteration_dictionaries[1:]])\n",
    "    tregions = np.sum([i['t_regions'] for i in iteration_dictionaries[1:]])\n",
    "    coverage = np.sum([i['coverage'] for i in iteration_dictionaries[1:]])\n",
    "    # # Read the last line of the text file to extract coverage\n",
    "    # with open(last_text_file, 'r') as file:\n",
    "    #     lines = file.readlines()\n",
    "    #     last_line = lines[-1].strip()\n",
    "    #     coverage = float(last_line.split()[1])\n",
    "\n",
    "    # Group experiments by 'b' value\n",
    "    if b not in grouped_experiments:\n",
    "        grouped_experiments[b] = {}\n",
    "    \n",
    "    # Group experiments by 'N' value\n",
    "    if N not in grouped_experiments[b]:\n",
    "        grouped_experiments[b][N] = []\n",
    "    \n",
    "    # Add the experiment data to the grouped experiments dictionary\n",
    "    grouped_experiments[b][N].append([\n",
    "            np.sum(len(g) for g in data['rb']), coverage , data['ttotal'], tsample, tvis, tmhs, tregions])\n",
    "\n",
    "# Print the grouped experiments\n",
    "for b, experiments in grouped_experiments.items():\n",
    "    print(f\"Experiments with b = {b}:\")\n",
    "    for N, experiment_data in experiments.items():\n",
    "        print(f\"  Experiments with N = {N}:\")\n",
    "        for dat in experiment_data:\n",
    "            print(f\"    Experiment data: {dat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: regions 6.7 time: 39.0\n",
      "std: regions 0.5 time: 7.4\n"
     ]
    }
   ],
   "source": [
    "data = np.array(grouped_experiments[1][500])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vg', 'vad', 'sp', 'ra', 'rb', 'tstep', 'tsample', 'tccv', 'ttotal'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: regions 10.4 time: 203.4\n",
      "std: regions 1.9 time: 29.0\n"
     ]
    }
   ],
   "source": [
    "data = np.array(grouped_experiments[1][1])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "def parse_logfile(filename):\n",
    "    logfile = open(filename, 'r')\n",
    "    lines = logfile.readlines()\n",
    "    logfile.close()\n",
    "\n",
    "    iteration_data = []\n",
    "    current_iteration = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('ITERATION:'):\n",
    "            if current_iteration:\n",
    "                iteration_data.append(current_iteration)\n",
    "                current_iteration = {}\n",
    "        elif line.startswith('summary'):\n",
    "            current_iteration['summary'] = line.strip()\n",
    "        elif line.startswith('number of regions step'):\n",
    "            current_iteration['number_of_regions_step'] = int(line.split()[-1])\n",
    "        elif line.startswith('number of regions total'):\n",
    "            current_iteration['number_of_regions_total'] = int(line.split()[-1])\n",
    "        elif line.startswith('tstep'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tstep'] = float(a.split()[1])\n",
    "            current_iteration['t_total'] = float(a.split()[3])\n",
    "        elif line.startswith('tsample'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tsample'] = float(a.split()[1])\n",
    "            current_iteration['t_visgraph'] = float(a.split()[3])\n",
    "        elif line.startswith('t_mhs'):\n",
    "            current_iteration['t_mhs'] = float(line.split()[1])\n",
    "        elif line.startswith('t_regions'):\n",
    "            current_iteration['t_regions'] = float(line.split()[1])\n",
    "        elif line.startswith('coverage'):\n",
    "            current_iteration['coverage'] = float(line.split()[1])\n",
    "        elif '[VisSeeder] Coverage met, terminated' in line:\n",
    "            current_iteration['termination'] = line.strip()\n",
    "\n",
    "    if current_iteration:\n",
    "        iteration_data.append(current_iteration)\n",
    "    return iteration_data\n",
    "\n",
    "\n",
    "logs_directory = '../logs'\n",
    "\n",
    "# Get a list of all subdirectories in the logs directory\n",
    "experiment_directories = [directory for directory in glob.glob(os.path.join(logs_directory, '*')) if os.path.isdir(directory)]\n",
    "\n",
    "# Define a dictionary to store the grouped experiments\n",
    "grouped_experiments = {}\n",
    "\n",
    "# Iterate over each experiment directory\n",
    "for experiment_directory in experiment_directories:\n",
    "    # Parse the experiment name\n",
    "    experiment_name = os.path.basename(experiment_directory)\n",
    "    name_parts = experiment_name.split('_')\n",
    "    \n",
    "    # Extract the relevant values from the experiment name\n",
    "    if '5dof_ur_naive' in experiment_name:\n",
    "        b = 1  #int(name_parts[-5])\n",
    "    else:\n",
    "        continue\n",
    "    N = int(name_parts[-3])\n",
    "    \n",
    "    # Get the last pickle file in the 'data' subdirectory\n",
    "    data_directory = os.path.join(experiment_directory, 'data')\n",
    "    pickle_files = glob.glob(os.path.join(data_directory, '*.pkl'))\n",
    "    pkl_files = os.listdir(data_directory)\n",
    "    pkl_idx = np.argsort([int(s.replace('it_', '').replace('.pkl', '')) for s in pkl_files])\n",
    "    \n",
    "    #pickle_files.sort()\n",
    "    \n",
    "    last_pickle_file = data_directory+'/'+pkl_files[pkl_idx[-1]]#pickle_files[-1]#max(pickle_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the last pickle file\n",
    "    with open(last_pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Find the 'summar' subdirectory and get the path to the text file\n",
    "    summar_directory = os.path.join(experiment_directory, 'summary')\n",
    "    text_files = glob.glob(os.path.join(summar_directory, '*.txt'))\n",
    "    last_text_file = max(text_files, key=os.path.getctime)\n",
    "    \n",
    "\n",
    "    iteration_dictionaries = parse_logfile(last_text_file)\n",
    "    tsample = np.sum([i['tsample'] for i in iteration_dictionaries[1:]])\n",
    "    tvis = np.sum([i['t_visgraph'] for i in iteration_dictionaries[1:]])\n",
    "    tmhs = np.sum([i['t_mhs'] for i in iteration_dictionaries[1:]])\n",
    "    tregions = np.sum([i['t_regions'] for i in iteration_dictionaries[1:]])\n",
    "    # # Read the last line of the text file to extract coverage\n",
    "    # with open(last_text_file, 'r') as file:\n",
    "    #     lines = file.readlines()\n",
    "    #     last_line = lines[-1].strip()\n",
    "    #     coverage = float(last_line.split()[1])\n",
    "\n",
    "    # Group experiments by 'b' value\n",
    "    if b not in grouped_experiments:\n",
    "        grouped_experiments[b] = {}\n",
    "    \n",
    "    # Group experiments by 'N' value\n",
    "    if N not in grouped_experiments[b]:\n",
    "        grouped_experiments[b][N] = []\n",
    "    \n",
    "    # Add the experiment data to the grouped experiments dictionary\n",
    "    grouped_experiments[b][N].append([\n",
    "            np.sum(len(g) for g in data['rb']), data['cov'] , data['ttotal'], tsample, tvis, tmhs, tregions])\n",
    "\n",
    "# Print the grouped experiments\n",
    "for b, experiments in grouped_experiments.items():\n",
    "    print(f\"Experiments with b = {b}:\")\n",
    "    for N, experiment_data in experiments.items():\n",
    "        print(f\"  Experiments with N = {N}:\")\n",
    "        for dat in experiment_data:\n",
    "            print(f\"    Experiment data: {dat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(grouped_experiments[1][1])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
