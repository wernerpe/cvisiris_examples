{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-415247a41d28>:123: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  np.sum(len(g) for g in data['rb']), coverage , data['ttotal'], tsample, tvis, tmhs, tregions])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-415247a41d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m     \u001b[0miteration_dictionaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_logfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_text_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0mtsample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tsample'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteration_dictionaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mtvis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_visgraph'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miteration_dictionaries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-415247a41d28>\u001b[0m in \u001b[0;36mparse_logfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mcurrent_iteration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tsample'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mcurrent_iteration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_visgraph'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mcurrent_iteration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_mhs'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             \u001b[0mcurrent_iteration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't_regions'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "def parse_logfile(filename):\n",
    "    logfile = open(filename, 'r')\n",
    "    lines = logfile.readlines()\n",
    "    logfile.close()\n",
    "\n",
    "    iteration_data = []\n",
    "    current_iteration = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('ITERATION:'):\n",
    "            if current_iteration:\n",
    "                iteration_data.append(current_iteration)\n",
    "                current_iteration = {}\n",
    "        elif line.startswith('summary'):\n",
    "            current_iteration['summary'] = line.strip()\n",
    "        elif line.startswith('number of regions step'):\n",
    "            current_iteration['number_of_regions_step'] = int(line.split()[-1])\n",
    "        elif line.startswith('number of regions total'):\n",
    "            current_iteration['number_of_regions_total'] = int(line.split()[-1])\n",
    "        elif line.startswith('tstep'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tstep'] = float(a.split()[1])\n",
    "            current_iteration['t_total'] = float(a.split()[3])\n",
    "        elif line.startswith('tsample'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tsample'] = float(a.split()[1])\n",
    "            current_iteration['t_visgraph'] = float(a.split()[3])\n",
    "            current_iteration['t_mhs'] = float(a.split()[5])\n",
    "            current_iteration['t_regions'] = float(a.split()[7])\n",
    "\n",
    "        # elif line.startswith('t_mhs'):\n",
    "        #     current_iteration['t_mhs'] = float(line.split()[1])\n",
    "        # elif line.startswith('t_regions'):\n",
    "        #     current_iteration['t_regions'] = float(line.split()[1])\n",
    "        elif line.startswith('coverage'):\n",
    "            current_iteration['coverage'] = float(line.split()[1])\n",
    "        elif '[VisSeeder] Coverage met, terminated' in line:\n",
    "            current_iteration['termination'] = line.strip()\n",
    "\n",
    "    if current_iteration:\n",
    "        iteration_data.append(current_iteration)\n",
    "    return iteration_data\n",
    "\n",
    "\n",
    "logs_directory = '../logs'\n",
    "\n",
    "# Get a list of all subdirectories in the logs directory\n",
    "experiment_directories = [directory for directory in glob.glob(os.path.join(logs_directory, '*')) if os.path.isdir(directory)]\n",
    "\n",
    "# Define a dictionary to store the grouped experiments\n",
    "grouped_experiments = {}\n",
    "\n",
    "# Iterate over each experiment directory\n",
    "for experiment_directory in experiment_directories:\n",
    "    # Parse the experiment name\n",
    "    experiment_name = os.path.basename(experiment_directory)\n",
    "    name_parts = experiment_name.split('_')\n",
    "    \n",
    "    # Extract the relevant values from the experiment name\n",
    "    if '3dof' in experiment_name:\n",
    "        b = 1  #int(name_parts[-5])\n",
    "        # print(experiment_name)\n",
    "    if '3dof_flipper2' in experiment_name:\n",
    "        b = 6  #int(name_parts[-5])\n",
    "        # print(experiment_name)\n",
    "    elif '5dof' in experiment_name and 'greedy' in experiment_name:\n",
    "        b = 2\n",
    "    elif '7dof_iiwa' in experiment_name and 'greedy' in experiment_name:\n",
    "        b = 3\n",
    "    else:\n",
    "        continue\n",
    "    N = int(name_parts[-2])\n",
    "    \n",
    "    # Get the last pickle file in the 'data' subdirectory\n",
    "    data_directory = os.path.join(experiment_directory, 'data')\n",
    "    pickle_files = glob.glob(os.path.join(data_directory, '*.pkl'))\n",
    "    pkl_files = os.listdir(data_directory)\n",
    "    pkl_idx = np.argsort([int(s.replace('it_', '').replace('.pkl', '')) for s in pkl_files])\n",
    "    \n",
    "    #pickle_files.sort()\n",
    "    \n",
    "    last_pickle_file = data_directory+'/'+pkl_files[pkl_idx[-1]]#pickle_files[-1]#max(pickle_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the last pickle file\n",
    "    with open(last_pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Find the 'summar' subdirectory and get the path to the text file\n",
    "    summar_directory = os.path.join(experiment_directory, 'summary')\n",
    "    text_files = glob.glob(os.path.join(summar_directory, '*.txt'))\n",
    "    last_text_file = max(text_files, key=os.path.getctime)\n",
    "    \n",
    "\n",
    "    iteration_dictionaries = parse_logfile(last_text_file)\n",
    "    tsample = np.sum([i['tsample'] for i in iteration_dictionaries[1:]])\n",
    "    tvis = np.sum([i['t_visgraph'] for i in iteration_dictionaries[1:]])\n",
    "    tmhs = np.sum([i['t_mhs'] for i in iteration_dictionaries[1:]])\n",
    "    tregions = np.sum([i['t_regions'] for i in iteration_dictionaries[1:]])\n",
    "    coverage = np.sum([i['coverage'] for i in iteration_dictionaries[1:]])\n",
    "    # # Read the last line of the text file to extract coverage\n",
    "    # with open(last_text_file, 'r') as file:\n",
    "    #     lines = file.readlines()\n",
    "    #     last_line = lines[-1].strip()\n",
    "    #     coverage = float(last_line.split()[1])\n",
    "\n",
    "    # Group experiments by 'b' value\n",
    "    if b not in grouped_experiments:\n",
    "        grouped_experiments[b] = {}\n",
    "    \n",
    "    # Group experiments by 'N' value\n",
    "    if N not in grouped_experiments[b]:\n",
    "        grouped_experiments[b][N] = []\n",
    "    \n",
    "    # Add the experiment data to the grouped experiments dictionary\n",
    "    grouped_experiments[b][N].append([\n",
    "            np.sum(len(g) for g in data['rb']), coverage , data['ttotal'], tsample, tvis, tmhs, tregions])\n",
    "\n",
    "# Print the grouped experiments\n",
    "for b, experiments in grouped_experiments.items():\n",
    "    print(f\"Experiments with b = {b}:\")\n",
    "    for N, experiment_data in experiments.items():\n",
    "        print(f\"  Experiments with N = {N}:\")\n",
    "        for dat in experiment_data:\n",
    "            print(f\"    Experiment data: {dat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: regions 44.5 time: 5425.1\n",
      "std: regions 7.2 time: 1911.2\n"
     ]
    }
   ],
   "source": [
    "data = np.array(grouped_experiments[3][1500])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['vg', 'vad', 'sp', 'ra', 'rb', 'tstep', 'tsample', 'tccv', 'ttotal'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: regions 10.4 time: 203.4\n",
      "std: regions 1.9 time: 29.0\n"
     ]
    }
   ],
   "source": [
    "data = np.array(grouped_experiments[1][1])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "def parse_logfile(filename):\n",
    "    logfile = open(filename, 'r')\n",
    "    lines = logfile.readlines()\n",
    "    logfile.close()\n",
    "\n",
    "    iteration_data = []\n",
    "    current_iteration = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('ITERATION:'):\n",
    "            if current_iteration:\n",
    "                iteration_data.append(current_iteration)\n",
    "                current_iteration = {}\n",
    "        elif line.startswith('summary'):\n",
    "            current_iteration['summary'] = line.strip()\n",
    "        elif line.startswith('number of regions step'):\n",
    "            current_iteration['number_of_regions_step'] = int(line.split()[-1])\n",
    "        elif line.startswith('number of regions total'):\n",
    "            current_iteration['number_of_regions_total'] = int(line.split()[-1])\n",
    "        elif line.startswith('tstep'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tstep'] = float(a.split()[1])\n",
    "            current_iteration['t_total'] = float(a.split()[3])\n",
    "        elif line.startswith('tsample'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tsample'] = float(a.split()[1])\n",
    "            current_iteration['t_visgraph'] = float(a.split()[3])\n",
    "        elif line.startswith('t_mhs'):\n",
    "            current_iteration['t_mhs'] = float(line.split()[1])\n",
    "        elif line.startswith('t_regions'):\n",
    "            current_iteration['t_regions'] = float(line.split()[1])\n",
    "        elif line.startswith('coverage'):\n",
    "            current_iteration['coverage'] = float(line.split()[1])\n",
    "        elif '[VisSeeder] Coverage met, terminated' in line:\n",
    "            current_iteration['termination'] = line.strip()\n",
    "\n",
    "    if current_iteration:\n",
    "        iteration_data.append(current_iteration)\n",
    "    return iteration_data\n",
    "\n",
    "\n",
    "logs_directory = '../logs'\n",
    "\n",
    "# Get a list of all subdirectories in the logs directory\n",
    "experiment_directories = [directory for directory in glob.glob(os.path.join(logs_directory, '*')) if os.path.isdir(directory)]\n",
    "\n",
    "# Define a dictionary to store the grouped experiments\n",
    "grouped_experiments = {}\n",
    "\n",
    "# Iterate over each experiment directory\n",
    "for experiment_directory in experiment_directories:\n",
    "    # Parse the experiment name\n",
    "    experiment_name = os.path.basename(experiment_directory)\n",
    "    name_parts = experiment_name.split('_')\n",
    "    \n",
    "    # Extract the relevant values from the experiment name\n",
    "    if '5dof_ur_naive' in experiment_name:\n",
    "        b = 1  #int(name_parts[-5])\n",
    "    else:\n",
    "        continue\n",
    "    N = int(name_parts[-3])\n",
    "    \n",
    "    # Get the last pickle file in the 'data' subdirectory\n",
    "    data_directory = os.path.join(experiment_directory, 'data')\n",
    "    pickle_files = glob.glob(os.path.join(data_directory, '*.pkl'))\n",
    "    pkl_files = os.listdir(data_directory)\n",
    "    pkl_idx = np.argsort([int(s.replace('it_', '').replace('.pkl', '')) for s in pkl_files])\n",
    "    \n",
    "    #pickle_files.sort()\n",
    "    \n",
    "    last_pickle_file = data_directory+'/'+pkl_files[pkl_idx[-1]]#pickle_files[-1]#max(pickle_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the last pickle file\n",
    "    with open(last_pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Find the 'summar' subdirectory and get the path to the text file\n",
    "    summar_directory = os.path.join(experiment_directory, 'summary')\n",
    "    text_files = glob.glob(os.path.join(summar_directory, '*.txt'))\n",
    "    last_text_file = max(text_files, key=os.path.getctime)\n",
    "    \n",
    "\n",
    "    iteration_dictionaries = parse_logfile(last_text_file)\n",
    "    tsample = np.sum([i['tsample'] for i in iteration_dictionaries[1:]])\n",
    "    tvis = np.sum([i['t_visgraph'] for i in iteration_dictionaries[1:]])\n",
    "    tmhs = np.sum([i['t_mhs'] for i in iteration_dictionaries[1:]])\n",
    "    tregions = np.sum([i['t_regions'] for i in iteration_dictionaries[1:]])\n",
    "    # # Read the last line of the text file to extract coverage\n",
    "    # with open(last_text_file, 'r') as file:\n",
    "    #     lines = file.readlines()\n",
    "    #     last_line = lines[-1].strip()\n",
    "    #     coverage = float(last_line.split()[1])\n",
    "\n",
    "    # Group experiments by 'b' value\n",
    "    if b not in grouped_experiments:\n",
    "        grouped_experiments[b] = {}\n",
    "    \n",
    "    # Group experiments by 'N' value\n",
    "    if N not in grouped_experiments[b]:\n",
    "        grouped_experiments[b][N] = []\n",
    "    \n",
    "    # Add the experiment data to the grouped experiments dictionary\n",
    "    grouped_experiments[b][N].append([\n",
    "            np.sum(len(g) for g in data['rb']), data['cov'] , data['ttotal'], tsample, tvis, tmhs, tregions])\n",
    "\n",
    "# Print the grouped experiments\n",
    "for b, experiments in grouped_experiments.items():\n",
    "    print(f\"Experiments with b = {b}:\")\n",
    "    for N, experiment_data in experiments.items():\n",
    "        print(f\"  Experiments with N = {N}:\")\n",
    "        for dat in experiment_data:\n",
    "            print(f\"    Experiment data: {dat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(grouped_experiments[1][1])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
