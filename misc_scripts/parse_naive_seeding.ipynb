{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3DOf_pinball_naive_20230830160358_1_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830163025_9_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830162403_7_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830162018_6_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830161027_3_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830160733_2_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830155947_0_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830161720_5_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830161357_4_1_0.050_0.100\n",
      "3DOf_pinball_naive_20230830162755_8_1_0.050_0.100\n",
      "Experiments with b = 1:\n",
      "  Experiments with N = 1:\n",
      "    Experiment data: [10, 0.9092, 212.61109471321106, 24.682000000000002, 0.42300000000000004, 0.0, 187.506]\n",
      "    Experiment data: [12, 0.9148, 220.2237195968628, 32.092, 0.629, 0.0, 187.503]\n",
      "    Experiment data: [12, 0.9104, 229.6338722705841, 29.878999999999998, 0.598, 0.0, 199.155]\n",
      "    Experiment data: [11, 0.905, 221.8490002155304, 29.663, 0.549, 0.0, 191.638]\n",
      "    Experiment data: [11, 0.899, 207.983628988266, 27.076999999999998, 0.457, 0.0, 180.45199999999997]\n",
      "    Experiment data: [8, 0.9042, 170.64069437980652, 20.025, 0.299, 0.0, 150.317]\n",
      "    Experiment data: [14, 0.9144, 248.00497102737427, 36.774, 0.8160000000000001, 0.0, 210.412]\n",
      "    Experiment data: [9, 0.907, 175.21703338623047, 19.651000000000003, 0.338, 0.0, 155.22899999999998]\n",
      "    Experiment data: [9, 0.903, 200.51606035232544, 26.9, 0.46499999999999997, 0.0, 173.15099999999998]\n",
      "    Experiment data: [8, 0.893, 147.6109278202057, 17.27, 0.241, 0.0, 130.1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.8/site-packages/scipy/__init__.py:143: UserWarning: A NumPy version >=1.19.5 and <1.27.0 is required for this version of SciPy (detected version 1.17.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "<ipython-input-6-e20ef70a778e>:109: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  np.sum(len(g) for g in data['rb']), data['cov'] , data['ttotal'], tsample, tvis, tmhs, tregions])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "def parse_logfile(filename):\n",
    "    logfile = open(filename, 'r')\n",
    "    lines = logfile.readlines()\n",
    "    logfile.close()\n",
    "\n",
    "    iteration_data = []\n",
    "    current_iteration = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('ITERATION:'):\n",
    "            if current_iteration:\n",
    "                iteration_data.append(current_iteration)\n",
    "                current_iteration = {}\n",
    "        elif line.startswith('summary'):\n",
    "            current_iteration['summary'] = line.strip()\n",
    "        elif line.startswith('number of regions step'):\n",
    "            current_iteration['number_of_regions_step'] = int(line.split()[-1])\n",
    "        elif line.startswith('number of regions total'):\n",
    "            current_iteration['number_of_regions_total'] = int(line.split()[-1])\n",
    "        elif line.startswith('tstep'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tstep'] = float(a.split()[1])\n",
    "            current_iteration['t_total'] = float(a.split()[3])\n",
    "        elif line.startswith('tsample'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tsample'] = float(a.split()[1])\n",
    "            current_iteration['t_visgraph'] = float(a.split()[3])\n",
    "        elif line.startswith('t_mhs'):\n",
    "            current_iteration['t_mhs'] = float(line.split()[1])\n",
    "        elif line.startswith('t_regions'):\n",
    "            current_iteration['t_regions'] = float(line.split()[1])\n",
    "        elif line.startswith('coverage'):\n",
    "            current_iteration['coverage'] = float(line.split()[1])\n",
    "        elif '[VisSeeder] Coverage met, terminated' in line:\n",
    "            current_iteration['termination'] = line.strip()\n",
    "\n",
    "    if current_iteration:\n",
    "        iteration_data.append(current_iteration)\n",
    "    return iteration_data\n",
    "\n",
    "\n",
    "logs_directory = '../logs'\n",
    "\n",
    "# Get a list of all subdirectories in the logs directory\n",
    "experiment_directories = [directory for directory in glob.glob(os.path.join(logs_directory, '*')) if os.path.isdir(directory)]\n",
    "\n",
    "# Define a dictionary to store the grouped experiments\n",
    "grouped_experiments = {}\n",
    "\n",
    "# Iterate over each experiment directory\n",
    "for experiment_directory in experiment_directories:\n",
    "    # Parse the experiment name\n",
    "    experiment_name = os.path.basename(experiment_directory)\n",
    "    name_parts = experiment_name.split('_')\n",
    "    \n",
    "    # Extract the relevant values from the experiment name\n",
    "    if '3DOf_pinball_naive' in experiment_name:\n",
    "        b = 1  #int(name_parts[-5])\n",
    "        print(experiment_name)\n",
    "    else:\n",
    "        continue\n",
    "    N = int(name_parts[-3])\n",
    "    \n",
    "    # Get the last pickle file in the 'data' subdirectory\n",
    "    data_directory = os.path.join(experiment_directory, 'data')\n",
    "    pickle_files = glob.glob(os.path.join(data_directory, '*.pkl'))\n",
    "    pkl_files = os.listdir(data_directory)\n",
    "    pkl_idx = np.argsort([int(s.replace('it_', '').replace('.pkl', '')) for s in pkl_files])\n",
    "    \n",
    "    #pickle_files.sort()\n",
    "    \n",
    "    last_pickle_file = data_directory+'/'+pkl_files[pkl_idx[-1]]#pickle_files[-1]#max(pickle_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the last pickle file\n",
    "    with open(last_pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Find the 'summar' subdirectory and get the path to the text file\n",
    "    summar_directory = os.path.join(experiment_directory, 'summary')\n",
    "    text_files = glob.glob(os.path.join(summar_directory, '*.txt'))\n",
    "    last_text_file = max(text_files, key=os.path.getctime)\n",
    "    \n",
    "\n",
    "    iteration_dictionaries = parse_logfile(last_text_file)\n",
    "    tsample = np.sum([i['tsample'] for i in iteration_dictionaries[1:]])\n",
    "    tvis = np.sum([i['t_visgraph'] for i in iteration_dictionaries[1:]])\n",
    "    tmhs = np.sum([i['t_mhs'] for i in iteration_dictionaries[1:]])\n",
    "    tregions = np.sum([i['t_regions'] for i in iteration_dictionaries[1:]])\n",
    "    # # Read the last line of the text file to extract coverage\n",
    "    # with open(last_text_file, 'r') as file:\n",
    "    #     lines = file.readlines()\n",
    "    #     last_line = lines[-1].strip()\n",
    "    #     coverage = float(last_line.split()[1])\n",
    "\n",
    "    # Group experiments by 'b' value\n",
    "    if b not in grouped_experiments:\n",
    "        grouped_experiments[b] = {}\n",
    "    \n",
    "    # Group experiments by 'N' value\n",
    "    if N not in grouped_experiments[b]:\n",
    "        grouped_experiments[b][N] = []\n",
    "    \n",
    "    # Add the experiment data to the grouped experiments dictionary\n",
    "    grouped_experiments[b][N].append([\n",
    "            np.sum(len(g) for g in data['rb']), data['cov'] , data['ttotal'], tsample, tvis, tmhs, tregions])\n",
    "\n",
    "# Print the grouped experiments\n",
    "for b, experiments in grouped_experiments.items():\n",
    "    print(f\"Experiments with b = {b}:\")\n",
    "    for N, experiment_data in experiments.items():\n",
    "        print(f\"  Experiments with N = {N}:\")\n",
    "        for dat in experiment_data:\n",
    "            print(f\"    Experiment data: {dat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: regions 10.4 time: 203.4\n",
      "std: regions 1.9 time: 29.0\n"
     ]
    }
   ],
   "source": [
    "data = np.array(grouped_experiments[1][1])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "def parse_logfile(filename):\n",
    "    logfile = open(filename, 'r')\n",
    "    lines = logfile.readlines()\n",
    "    logfile.close()\n",
    "\n",
    "    iteration_data = []\n",
    "    current_iteration = {}\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith('ITERATION:'):\n",
    "            if current_iteration:\n",
    "                iteration_data.append(current_iteration)\n",
    "                current_iteration = {}\n",
    "        elif line.startswith('summary'):\n",
    "            current_iteration['summary'] = line.strip()\n",
    "        elif line.startswith('number of regions step'):\n",
    "            current_iteration['number_of_regions_step'] = int(line.split()[-1])\n",
    "        elif line.startswith('number of regions total'):\n",
    "            current_iteration['number_of_regions_total'] = int(line.split()[-1])\n",
    "        elif line.startswith('tstep'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tstep'] = float(a.split()[1])\n",
    "            current_iteration['t_total'] = float(a.split()[3])\n",
    "        elif line.startswith('tsample'):\n",
    "            a = line.replace(',', '')\n",
    "            current_iteration['tsample'] = float(a.split()[1])\n",
    "            current_iteration['t_visgraph'] = float(a.split()[3])\n",
    "        elif line.startswith('t_mhs'):\n",
    "            current_iteration['t_mhs'] = float(line.split()[1])\n",
    "        elif line.startswith('t_regions'):\n",
    "            current_iteration['t_regions'] = float(line.split()[1])\n",
    "        elif line.startswith('coverage'):\n",
    "            current_iteration['coverage'] = float(line.split()[1])\n",
    "        elif '[VisSeeder] Coverage met, terminated' in line:\n",
    "            current_iteration['termination'] = line.strip()\n",
    "\n",
    "    if current_iteration:\n",
    "        iteration_data.append(current_iteration)\n",
    "    return iteration_data\n",
    "\n",
    "\n",
    "logs_directory = '../logs'\n",
    "\n",
    "# Get a list of all subdirectories in the logs directory\n",
    "experiment_directories = [directory for directory in glob.glob(os.path.join(logs_directory, '*')) if os.path.isdir(directory)]\n",
    "\n",
    "# Define a dictionary to store the grouped experiments\n",
    "grouped_experiments = {}\n",
    "\n",
    "# Iterate over each experiment directory\n",
    "for experiment_directory in experiment_directories:\n",
    "    # Parse the experiment name\n",
    "    experiment_name = os.path.basename(experiment_directory)\n",
    "    name_parts = experiment_name.split('_')\n",
    "    \n",
    "    # Extract the relevant values from the experiment name\n",
    "    if '5dof_ur_naive' in experiment_name:\n",
    "        b = 1  #int(name_parts[-5])\n",
    "    else:\n",
    "        continue\n",
    "    N = int(name_parts[-3])\n",
    "    \n",
    "    # Get the last pickle file in the 'data' subdirectory\n",
    "    data_directory = os.path.join(experiment_directory, 'data')\n",
    "    pickle_files = glob.glob(os.path.join(data_directory, '*.pkl'))\n",
    "    pkl_files = os.listdir(data_directory)\n",
    "    pkl_idx = np.argsort([int(s.replace('it_', '').replace('.pkl', '')) for s in pkl_files])\n",
    "    \n",
    "    #pickle_files.sort()\n",
    "    \n",
    "    last_pickle_file = data_directory+'/'+pkl_files[pkl_idx[-1]]#pickle_files[-1]#max(pickle_files, key=os.path.getctime)\n",
    "    \n",
    "    # Load the last pickle file\n",
    "    with open(last_pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # Find the 'summar' subdirectory and get the path to the text file\n",
    "    summar_directory = os.path.join(experiment_directory, 'summary')\n",
    "    text_files = glob.glob(os.path.join(summar_directory, '*.txt'))\n",
    "    last_text_file = max(text_files, key=os.path.getctime)\n",
    "    \n",
    "\n",
    "    iteration_dictionaries = parse_logfile(last_text_file)\n",
    "    tsample = np.sum([i['tsample'] for i in iteration_dictionaries[1:]])\n",
    "    tvis = np.sum([i['t_visgraph'] for i in iteration_dictionaries[1:]])\n",
    "    tmhs = np.sum([i['t_mhs'] for i in iteration_dictionaries[1:]])\n",
    "    tregions = np.sum([i['t_regions'] for i in iteration_dictionaries[1:]])\n",
    "    # # Read the last line of the text file to extract coverage\n",
    "    # with open(last_text_file, 'r') as file:\n",
    "    #     lines = file.readlines()\n",
    "    #     last_line = lines[-1].strip()\n",
    "    #     coverage = float(last_line.split()[1])\n",
    "\n",
    "    # Group experiments by 'b' value\n",
    "    if b not in grouped_experiments:\n",
    "        grouped_experiments[b] = {}\n",
    "    \n",
    "    # Group experiments by 'N' value\n",
    "    if N not in grouped_experiments[b]:\n",
    "        grouped_experiments[b][N] = []\n",
    "    \n",
    "    # Add the experiment data to the grouped experiments dictionary\n",
    "    grouped_experiments[b][N].append([\n",
    "            np.sum(len(g) for g in data['rb']), data['cov'] , data['ttotal'], tsample, tvis, tmhs, tregions])\n",
    "\n",
    "# Print the grouped experiments\n",
    "for b, experiments in grouped_experiments.items():\n",
    "    print(f\"Experiments with b = {b}:\")\n",
    "    for N, experiment_data in experiments.items():\n",
    "        print(f\"  Experiments with N = {N}:\")\n",
    "        for dat in experiment_data:\n",
    "            print(f\"    Experiment data: {dat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(grouped_experiments[1][1])\n",
    "means = data.mean(axis =0)\n",
    "std = data.std(axis =0)\n",
    "\n",
    "print(f\"mean: regions {means[0]:.1f} time: {means[2]:.1f}\")\n",
    "print(f\"std: regions {std[0]:.1f} time: {std[2]:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
